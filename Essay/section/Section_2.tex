\chapter{Temp}
\label{sec:lab}
\section{Preprocessing of NoReC}
The NoReC dataset consists of over 43.000 full-text reviews as raw text. It also contains a metadata-file in json-format. Each review's filename is six digits, corresponding to the review's metadata in the json-file. The original dataset has all reviews divided into training set, development set and test set. To more easily conduct domain adaptation experiments, it would be sensible to have the reviews sorted by domains while keeping the original split in the data. Further, as the reviews are in a raw text format, we need to tokenize them. To achieve this, I used NLTK's pre-trained PunktSentenceTokenizer for Norwegian, as a basis for NLTK's word\_tokenize(). I iterated through all reviews in their respective split, found their rating and category in the metadata, and created CSV-files with "rating,text" as headers, dividing them into folders based on category. This way we get to keep the train, test and development split, while we're able to easily use the rating for each review as a label. Further, we would not want to read every review each time we ran an experiment for a given category, so I appended all reviews in each category to a pandas dataframe, and stored them as a pickle-file.
\section{Initial Experiments with Bag of Words and TF-IDF}
To create a baseline for domain adaptation on the NoReC dataset, I started conducting experiments with Scikit Learn's TfidfVectorizer and C-Support Vector Classification. By default, the TfidfVectorizer converts all characters to lowercase, which we might want to try without at a later point, ngram range to 1,1 and L2 normalization. For the C-Support Vector Classification, we want to change the kernel to linear, rather than the radial basis function. We then proceed to train on one domain before testing it on the other domains and repeat this for all nine domains.\\
The second experiment I wanted to conduct, was to train the model on all of the training data before testing on separate domains, and to trian the model on separate domains before testing it on all the test data as one test set. As we can see from table \ref{tab:bow} we get fairly decent results from a baseline classifier, where six out of our nine domains has the highest accuracy when tested on their respective test set.\\
The table is sorted by the amount of data, with Screen beeing the highest and Sports the lowest, with the reported accuracy for training/testing on all domains as one dataset is reported last. As for the outliers in the table, Misc yields a better score when tested on Stage. This might be because Misc contains reviews belonging to one of the other categories, being the thrid largest dataset. This suspicion is further backed up by the fact that Games yields a higher test score when tested on Misc than on Games. This is something that should be looked further into. Sports is as of now preforming a lot worse than any other domain, most likely due to the low amounts of data we have for Sports. Another reason for the low accuracy on the Sports domain, is the average length of the reviews in the test data. As mentioned in subsection \ref{NoReC} the average length of the reviews in the Sports domain is almost half of the length in its training data (266.9 versus 503.9), and significantly shorter than the other domains. Further, if we look at the Music domain, we can see that the average length of reviews in the test data is only 385.7, which is not that much longer than the average sports review. However, this is closer to the average length of its training data, and Music is also the second largest domain. This suggests that Sports having a short average review length in its test data alone, is not enough to justify its low accuracy. \\

\begin{table}[]
	\centering
	\resizebox{\columnwidth}{!}{%
	\begin{tabular}{@{}lrrrrrrrrrrr@{}}
		\toprule
		Train/Test                      & \multicolumn{1}{c}{Screen} & \multicolumn{1}{c}{Music} & \multicolumn{1}{c}{Misc} & \multicolumn{1}{c}{Literature} & \multicolumn{1}{c}{Products} & \multicolumn{1}{c}{Games} & \multicolumn{1}{c}{Rest.} & \multicolumn{1}{c}{Stage} & \multicolumn{1}{c}{Sports} & \multicolumn{1}{c}{All} & \multicolumn{1}{c}{Avg.} \\ \midrule
		\multicolumn{1}{l}{Screen}     & \textbf{57.45}             & 48.06                      & 50.75                    & 49.76                   & 38.84                        & 48.60                     & 54.94                     & 50.66                     & 22.72                      & 50.95 & \textbf{45.54}              \\
		\multicolumn{1}{l}{Music}      & 51.50                      & \textbf{56.02}             & 51.84                    & 46.74                          & 42.60                        & 47.48                     & 46.15                     & 49.33                     & 27.27                      & 51.29        &  44,73         \\
		\multicolumn{1}{l}{Misc}       & 44.22                      & 49.65                     & 54.01                     & 52.09                          & 49.56                        & 44.13                     & 27.47                     & \textbf{56.00}             & 22.72                      & 47.85   & 43.23                \\
		\multicolumn{1}{l}{Literature} & 40.65                      & 46.47                     & 50.75                    & \textbf{54.41}                 & 39.42                        & 43.01                     & 39.56                     & 48.00                      & 22.72                      & 44.86   & 41.32                \\
		\multicolumn{1}{l}{Products}   & 32.82                      & 41.16                     & 48.37                    & 45.81                          & \textbf{50.43}               & 48.04                      & 23.07                      & 49.33                     & 9.09                      & 40.26  & 37.21                 \\
		\multicolumn{1}{l}{Games}      & 36.31                      & 43.36                     & \textbf{50.75}           & 44.65                          & 49.85                        & 48.04                      & 23.07                      & 48.00                      & 9.09                       & 42.15  & 38.13                \\
		\multicolumn{1}{l}{Rest.}      & 38.34                      & 31.84                     & 31.01                     & 33.25                          & 25.79                        & 27.37                     & \textbf{45.05}             & 32.00                      & 18.18                      & 33.50    & 27.72              \\
		\multicolumn{1}{l}{Stage}      & 34.07                       & 42.83                     & 48.80                    & 45.81                          & 49.85                        & 46.36                     & 26.37                     & \textbf{53.33}            & 27.27                      & 41.34     & 40.17             \\
		\multicolumn{1}{l}{Sports}     & \textbf{20.22}                      & 15.92                      & 13.44                    & 11.86                          & 9.56                        & 12.84                     & 21.97                      & 6.66                     & 13.63             & 15.99  & 14.05                  \\
		\multicolumn{1}{l}{All}        & 57.80                      & 56.55                     & \textbf{58.35}            & 57.90                          & 53.33                        & 50.27                     & 57.14                     & 57.33                     & 4.54                      & 56.53 & - \\
		\multicolumn{1}{l}{Avg.}        & 37.26                      & 39.91                     & \textbf{43.21}            & 41.24                          & 38.18                       & 39.72                     & 32.82                     & 42.49                     & 19.26                      & 37.12    & -                  
	\end{tabular}%
}
	\caption{Bag of Words classifier with SVM and TF-IDF, with averaged out of domain predictions.}
	\label{tab:bow}
\end{table}

\section{NorBERT}
NorBERT [reference for NorBERT?] is a BERT language model \cite{devlin-etal-2019-bert}, trained for Norwegian. NorBERT features a custom 30.000 WordPiece vocabulary, and outperforms Google's multilingual BERT on most, if not all, Norwegian tasks. 
\subsection{NorBERT Baseline}
Replicate initial experiments with NorBERT

\subsection{Further experiments with NorBERT}
BERT only accepts 512 tokens, which means that we rarely get the entire review.
TODO:\\
Majority class classifier\\

