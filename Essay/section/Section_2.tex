\chapter{Temp}
\label{sec:lab}
\section{Preprocessing}
The NoReC dataset consists of over 43.000 full-text reviews as raw text. It also contains a metadata-file in json-format. Each review's filename is six digits, corresponding to the review's metadata in the json-file. The original dataset has all reviews divided into training set, development set and test set. To more easily conduct domain adaptation experiments, it would be sensible to have the reviews sorted by domains while keeping the original split in the data. Further, as the reviews are in a raw text format, we need to tokenize them. To achieve this, I used NLTK's pre-trained PunktSentenceTokenizer for Norwegian, as a basis for NLTK's word\_tokenize(). I iterated through all reviews in their respective split, found their rating and category in the metadata, and created CSV-files with "rating,text" as headers, dividing them into folders based on category. This way we get to keep the train, test and development split, while we're able to easily use the rating for each review as a label. Further, we would not want to read every review each time we ran an experiment for a given category, so I appended all reviews in each category to a pandas dataframe, and stored them as a pickle-file.
\section{Initial Experiments}
To create a baseline for domain adaptation on the NoReC dataset, I started conducting experiments with Scikit Learn's TfidfVectorizer and C-Support Vector Classification. By default, the TfidfVectorizer converts all characters to lowercase, which we might want to try without at a later point, ngram range to 1,1 and L2 normalization. For the C-Support Vector Classification, we want to change the kernel to linear, rather than the radial basis function. We then proceed to train on one domain before testing it on the other domains and repeat this for all nine domains. The second experiment I wanted to conduct, was to train the model on all of the training data before testing on separate domains, and to trian the model on separate domains before testing it on all the test data as one test set. As we can see from table \ref{tab:bow} we get fairly decent results from a baseline classifier, where six out of our nine domains has the highest accuracy when tested on their respective test set. The table is sorted by the amount of data, with Screen beeing the highest and Sports the lowest, with the reported accuracy for training/testing everything is reported last. As for the outliers in the table, Misc yields a better score when tested on Stage. This might be because Misc could use better sorting, being the thrid largest dataset. This suspicion is further backed up by the fact that Games yields a higher test score when tested on Misc than on Games. In other words, this is something that should be looked further into. Sports is as of now preforming a lot worse than any other domain, most likely due to the low amounts of data we have for Sports. After looking at a Sports review, I found that it barely had over 20 words in total, which also may indicate that the reviews in the domain is somewhat shorter than the the other domains. To look further into this, I conducted an experiment where I averaged the length of all reviews in different domains.\\

\begin{table}[]
	\centering
	\resizebox{\columnwidth}{!}{%
	\begin{tabular}{@{}lcccccccccc@{}}
		\toprule
		Train/Test                      & \multicolumn{1}{l}{Screen} & \multicolumn{1}{l}{Music} & \multicolumn{1}{l}{Misc} & \multicolumn{1}{l}{Literature} & \multicolumn{1}{l}{Products} & \multicolumn{1}{l}{Games} & \multicolumn{1}{l}{Rest.} & \multicolumn{1}{l}{Stage} & \multicolumn{1}{l}{Sports} & \multicolumn{1}{l}{All} \\ \midrule
		\multicolumn{1}{l|}{Screen}     & \textbf{57.45}             & 48.06                      & 50.75                    & 49.76                          & 38.84                        & 48.6                     & 54.94                     & 50.66                     & 22.72                      & 50.95                   \\
		\multicolumn{1}{l|}{Music}      & 51.50                      & \textbf{56.02}             & 51.84                    & 46.74                          & 42.60                        & 47.48                     & 46.15                     & 49.33                     & 27.27                      & 51.29                   \\
		\multicolumn{1}{l|}{Misc}       & 44.22                      & 49.65                     & 54.01                     & 52.09                          & 49.56                        & 44.13                     & 27.47                     & \textbf{56.0}             & 22.72                      & 47.85                   \\
		\multicolumn{1}{l|}{Literature} & 40.65                      & 46.47                     & 50.75                    & \textbf{54.41}                 & 39.42                        & 43.01                     & 39.56                     & 48.0                      & 22.72                      & 44.86                   \\
		\multicolumn{1}{l|}{Products}   & 32.82                      & 41.16                     & 48.37                    & 45.81                          & \textbf{50.43}               & 48.04                      & 23.07                      & 49.33                     & 9.09                      & 40.26                   \\
		\multicolumn{1}{l|}{Games}      & 36.31                      & 43.36                     & \textbf{50.75}           & 44.65                          & 49.85                        & 48.04                      & 23.07                      & 48.0                      & 9.09                       & 42.15                   \\
		\multicolumn{1}{l|}{Rest.}      & 38.34                      & 31.84                     & 31.01                     & 33.25                          & 25.79                        & 27.37                     & \textbf{45.05}             & 32.0                      & 18.18                      & 33.5                   \\
		\multicolumn{1}{l|}{Stage}      & 34.07                       & 42.83                     & 48.80                    & 45.81                          & 49.85                        & 46.36                     & 26.37                     & \textbf{53.33}            & 27.27                      & 41.34                   \\
		\multicolumn{1}{l|}{Sports}     & \textbf{20.22}                      & 15.92                      & 13.44                    & 11.86                          & 9.56                        & 12.84                     & 21.97                      & 6.66                     & 13.63             & 15.99                    \\
		\multicolumn{1}{l|}{All}        & 57.80                      & 56.55                     & \textbf{58.35}            & 57.90                          & 53.33                        & 50.27                     & 57.14                     & 57.33                     & 4.54                      & 56.53                       
	\end{tabular}%
}
	\caption{Bag of Words classifier with SVM and TF-IDF}
	\label{tab:bow}
\end{table}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\centering
	\resizebox{\columnwidth}{!}{%
	\begin{tabular}{@{}lccccccccc@{}}
		\toprule
		& \multicolumn{1}{l}{Screen} & \multicolumn{1}{l}{Music} & \multicolumn{1}{l}{Misc} & \multicolumn{1}{l}{Literature} & \multicolumn{1}{l}{Products} & \multicolumn{1}{l}{Games} & \multicolumn{1}{l}{Rest.} & \multicolumn{1}{l}{Stage} & \multicolumn{1}{l}{Sports} \\ \midrule
		\multicolumn{1}{l|}{Train} & 422.2                      & 325.9                     & 516.3                    & 445.5                          & 981                          & 569.4                     & 789.9                     & 567                       & 503.9                      \\
		\multicolumn{1}{l|}{Dev}   & 500.2                      & 406.5                     & 540.7                    & 584.4                          & 1082.5                       & 628.8                     & 832.8                     & 606.2                     & 165.4                      \\
		\multicolumn{1}{l|}{Test}  & 546.2                      & 385.7                     & 538.4                    & 579.9                          & 1008                         & 740.6                     & 894.7                     & 648.6                     & 266.9                     
	\end{tabular}%
}
	\caption{Average number of tokens per review for each domain in each split}
\end{table}
TODO:\\
Table analysis\\
Move number of tokens to background\\
Majority class classifier\\
Proxy A Distance\\
